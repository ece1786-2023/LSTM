{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Modified training process"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f13296ad77953aaa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# coding:utf-8\n",
    "'''\n",
    "**************************************************\n",
    "@File   ：LSTM -> train\n",
    "@IDE    ：PyCharm\n",
    "@Author ：Tianze Zhang\n",
    "@Desc   , \n",
    "@Date   ：2023/11/19 19:28\n",
    "**************************************************\n",
    "'''\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AdamW\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define a custom dataset class for your sentences\n",
    "class RimWordDS(Dataset):\n",
    "    def __init__(self, sentences, tokenizer, max_length=128):\n",
    "        self.sentences = sentences\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = str(self.sentences[idx])\n",
    "        inputs = self.tokenizer(sentence, return_tensors=\"pt\", max_length=self.max_length, truncation=True)\n",
    "        return inputs\n",
    "\n",
    "def lm_collate_fn(batch, device):\n",
    "    x = [item.data['input_ids'] for item in batch]  # List (len B) of varying lengths\n",
    "    y = [item.data['attention_mask'] for item in batch]  # List (len B) of the same lengths as x\n",
    "    maxlen = max([s.shape[1] for s in x])\n",
    "\n",
    "    padded_x, padded_y = [], []\n",
    "    for sx, sy in zip(x, y):\n",
    "        padded_x.append(torch.cat([sx.squeeze(), torch.ones(maxlen - sx.shape[1])]))\n",
    "        padded_y.append(torch.cat([sy.squeeze(), torch.ones(maxlen - sy.shape[1])]))\n",
    "    for i in range(len(batch)):\n",
    "        batch[i].data['input_ids'] = padded_x[i].reshape(1, -1)\n",
    "        batch[i].data['attention_mask'] = padded_y[i].reshape(1, -1)\n",
    "    return torch.stack(padded_x).long().to(device), torch.stack(padded_y).long().to(device)\n",
    "\n",
    "# Add special tokens\n",
    "token_name = \"[PAWN_nameDef]\"\n",
    "token_possessive = \"[PAWN_possessive]\"\n",
    "token_pronoun = \"[PAWN_pronoun]\"\n",
    "\n",
    "# Fine-tuning parameters\n",
    "epochs = 10\n",
    "learning_rate = 5e-6\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"using device \" + str(device))\n",
    "\n",
    "# the model to load from\n",
    "model_name_load = \"gpt2\"  # get a fresh gpt-2 model from hugging face\n",
    "# model_name_load=\"ft1\" #load a previously saved model\n",
    "\n",
    "# the model to save as\n",
    "model_name_save = \"ft1\"  # get a fresh gpt-2 model from hugging face\n",
    "\n",
    "# data_file_path=\"raw_data/backstory.csv\"\n",
    "data_file_path = \"raw_data/backstory.pkl\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_load)\n",
    "# TODO attention mask and the pad token id\n",
    "# The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
    "# Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_load).to(device)\n",
    "\n",
    "# data_file_path=\"backstory.pkl\"\n",
    "df = pd.read_pickle(data_file_path)\n",
    "titles = df[\"Title\"]\n",
    "sentences = df[\"Desc\"]\n",
    "sentences = sentences.tolist()\n",
    "len_data = len(titles)\n",
    "\n",
    "for i in range(len_data):\n",
    "    # print(i,type(sentences[i]),str(sentences[i]))\n",
    "    sentences[i] = \"This is the story of [PAWN_nameDef], a \" + titles[i] + \": \" + sentences[i]\n",
    "    # TODO attributes\n",
    "\n",
    "# convert backstory into proper prompt\n",
    "sentences_train, sentences_test = train_test_split(sentences, test_size=0.1, random_state=42)\n",
    "\n",
    "# Create a custom dataset\n",
    "dataset_train = RimWordDS(sentences_train, tokenizer)\n",
    "dataset_test = RimWordDS(sentences_test, tokenizer)\n",
    "\n",
    "# TODO cross validation?\n",
    "\n",
    "# Set up DataLoader\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=8, shuffle=True, collate_fn=lambda batch: lm_collate_fn(batch, device))\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=8, shuffle=True, collate_fn=lambda batch: lm_collate_fn(batch, device))\n",
    "# TODO padding to allow batching\n",
    "dataloader_train_len = len(dataloader_train)\n",
    "dataloader_test_len = len(dataloader_test)\n",
    "\n",
    "# Set up optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "# TODO optimizer selection\n",
    "\n",
    "loss_ot_train = np.zeros(epochs)\n",
    "loss_ot_test = np.zeros(epochs)\n",
    "\n",
    "# Fine-tune loop\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    total_loss_train = 0\n",
    "    total_loss_test = 0\n",
    "\n",
    "    progress_bar = tqdm(dataloader_train, desc=f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    for batch in progress_bar:\n",
    "        # Move batch to device\n",
    "        batch = {\"input_ids\":batch[0], \"attention_mask\":batch[1]}\n",
    "        # batch = {key: value[0].to(device) for key, value in batch.items()}\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(**batch, labels=batch[\"input_ids\"])\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss_train += loss.item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\"Loss\": total_loss_train / dataloader_train_len})\n",
    "\n",
    "    for batch_idx, batch in enumerate(dataloader_test, 1):\n",
    "        # Move batch to device\n",
    "        # batch = {key: value[0].to(device) for key, value in batch.items()}\n",
    "        batch = {\"input_ids\":batch[0], \"attention_mask\":batch[1]}\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(**batch, labels=batch[\"input_ids\"])\n",
    "        loss = outputs.loss\n",
    "\n",
    "        total_loss_test += loss.item()\n",
    "    total_loss_test /= dataloader_test_len\n",
    "    print(\"Epoch \", epoch, \"/\", epochs,\": Test loss=\", total_loss_test)\n",
    "\n",
    "    # record loss of the epoch\n",
    "    loss_ot_train[epoch] = total_loss_train / dataloader_train_len\n",
    "    loss_ot_test[epoch] = total_loss_test"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "initial_id"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plot loss"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6578714792dead01"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, splot = plt.subplots(1)\n",
    "domain = np.arange(epochs)\n",
    "splot.plot(domain, loss_ot_train, 'g',label=\"train\")\n",
    "splot.plot(domain, loss_ot_test, 'r',label=\"test\")\n",
    "splot.legend()\n",
    "splot.title.set_text(\"Loss over time\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7ac121a52a6c977d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
